{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\Documents\\GitHub\\Sign_Recognition\\py\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16916083113231338628\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15293416635148734156\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9883535296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11371599911884087136\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13239020461562938042\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "tf 2.2.0\n",
      "keras 2.3.0-tf\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n",
      "set_global_determinism(seed=1337) 이거 꼭 해라\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\yukir\\Documents\\GitHub\\Sign_Recognition\\py\n",
    "from basic_preprocessing import *\n",
    "from deeplearning_check import *\n",
    "from machine_learning import *\n",
    "\n",
    "gpu_check()\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if you are using a GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    #사이즈\n",
    "    plt.figure(figsize = (10,8))\n",
    "    #xticks/yticks - 눈금표\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #코랩에서 안돌아감 주의\n",
    "    plt.imshow(img, cmap= 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vid_info(path, codec, name):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    print(cap)\n",
    "    width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "    #재생할 파일의 높이 얻기\n",
    "    height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    #재생할 파일의 프레임 레이트 얻기\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    #codec\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    \n",
    "    #filename\n",
    "    filename = name+'.mp4'\n",
    "    \n",
    "    #out \n",
    "    out = cv2.VideoWriter(filename, fourcc, fps, (int(width), int(height)))\n",
    "    \n",
    "    print('cap {0}, width {1}, height {2}, fps {3}'.format(cap, width, height, fps))\n",
    "    print('codec {0}', 'filename {1}'.format(fourcc, filename))\n",
    "    \n",
    "    return cap, width, height, fps, fourcc, filename, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\yukir\\\\Documents\\\\Monicas_workspace\\\\pose_estimation\\\\IVE 아이브 'LOVE DIVE' DANCE PRACTICE.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VideoCapture 0000021FA1988590>\n",
      "cap <VideoCapture 0000021FA1988590>, width 1280.0, height 720.0, fps 23.976023976023978\n",
      "codec {0} filename test6.mp4\n"
     ]
    }
   ],
   "source": [
    "cap, width, height, fps, fourcc, filename, out = vid_info(path, 'XVID', 'test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to loop through each person detected and render\n",
    "\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280.0, 720.0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yukir\\Documents\\Monicas_workspace\\pose_estimation\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\yukir\\Documents\\Monicas_workspace\\pose_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(path)\n",
    "k = 0\n",
    "img_list = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    k+=1\n",
    "    if k<200:\n",
    "        # show(frame)\n",
    "        img_list.append(frame)\n",
    "        # break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = img_list[50:]\n",
    "# print(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "released\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"C:\\\\Users\\\\yukir\\\\Documents\\\\Monicas_workspace\\\\pose_estimation\\\\test2.mp4\")\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    k+=1\n",
    "    try: \n",
    "        if 50 <= k <=3000:\n",
    "            \n",
    "            # print(k)\n",
    "            # Resize image\n",
    "            img = frame.copy()\n",
    "            # show(img)\n",
    "            ## 사이즈 모델이 32의 배수여야하고 256pixel이상이여야함\n",
    "            ### ratio 따라서 reshape해야함 \n",
    "            img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 352,608)\n",
    "            input_img = tf.cast(img, dtype=tf.int32)\n",
    "        #\n",
    "            # Detection section\n",
    "            results = movenet(input_img)\n",
    "\n",
    "            #rendering\n",
    "            keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "            \n",
    "            # Render keypoints \n",
    "            loop_through_people(frame, keypoints_with_scores, EDGES, 0.1)\n",
    "            out.write(frame)\n",
    "\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "out.release()\n",
    "print('released')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d93d6df27dcc797823232b994e4f43d959f011089850837812bf48ca3e70a46"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
